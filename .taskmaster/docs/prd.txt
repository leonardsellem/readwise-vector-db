# Product Requirements Document (PRD)

## Project: **Readwiseâ€‘Vector Selfâ€‘Host Starter Pack**

### RevisionÂ history

| Date (EU)  | Version   | Author                           | Notes                |
| ---------- | --------- | -------------------------------- | -------------------- |
| 23â€‘06â€‘2025 | 0.1â€‘draft | Vibeâ€‘CodingÂ CopilotÂ \[AIâ€‘assist] | First complete draft |

---

## 1Â Purpose & Scope

Build a **fully selfâ€‘hosted application** that ingests all Readwise & ReadwiseÂ Reader highlights, converts them to dense embeddings using **OpenAIÂ ********************`textâ€‘embeddingâ€‘3â€‘large`******************** (3072â€‘D)**, stores them in **PostgreSQLÂ 16 + \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*`pgvector`**, and exposes a **FastAPI** service for semantic search. The solution must run within *freeâ€‘tier* limits (Docker Compose locally, GitHub Actions cron for nightly sync).

---

## 2Â Background

* Knowledge workers collect thousands of highlights across Kindle, articles, PDF, tweets, etc. via Readwise. ğŸ” Retrieving relevant snippets later is painful.
* Vector search over embeddings offers subâ€‘second semantic retrieval without fullâ€‘text constraints.
* Existing managed vector services (Pinecone, Supabase Pro) add cost; users prefer total ownership & offline portability.

---

## 3Â Goals & Success Criteria

| Â ID | Goal                                        | SuccessÂ metric                                          |
| --- | ------------------------------------------- | ------------------------------------------------------- |
| Â G1 | 100â€¯% backâ€‘fill of legacy highlights        | *N* highlights in DB == *N* reported by Readwise export |
| Â G2 | <â€¯24â€¯h lag for new highlights               | Cron sync completes nightly with Î”tÂ â‰¤Â 24â€¯h              |
| Â G3 | Subâ€‘500â€¯ms P95 search latency on laptop CPU | Locust test: 95â€¯thÂ â‰¤â€¯0.5â€¯s for 20 concurrent users      |
| Â G4 | â‰¤â€¯500Â MB total disk on fresh install        | `docker system df` postâ€‘import                          |
| Â G5 | Zero paid services                          | All infra runs on local Docker or free tiers            |

---

## 4Â Nonâ€‘Goals

* No web UI (CLI + JSON API only)
* No write/update UI for highlights (Readwise remains sourceâ€‘ofâ€‘truth)
* No summarization/chatbot features in v1
* Mobile distribution not included

---

## 5Â User Personas

1. \*\*Me\*\* for personal, professional and research use.

---

## 6Â Assumptions

* User possesses a valid **READWISE\_TOKEN**.
* Internet connectivity exists for Readwise & OpenAI API calls during sync.
* Local machine has â‰¥Â 4â€¯GB RAM & Docker.

---

## 7Â Functional Requirements

| Â FRÂ  | Description                                                                                                                                                          |
| ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| FR1  | **Backâ€‘fill ingest**: Fetch all historical highlights via `GET /api/v2/export/` (Readwise) & `GET /api/v3/list/` (Reader).                                           |
| FR2  | **Incremental sync**: Nightly job uses `updatedAfter=<last_cursor>` + pagination until `nextPageCursor=null`. Persist cursor in DB.                                  |
| FR3  | **Embedding generation**: Pipe each highlightâ€‘text to OpenAI `textâ€‘embeddingâ€‘3â€‘large`; handle 8192Â tokens limit with truncation warning.                             |
| FR4  | **Vector upsert**: `UPSERT` rows into `highlights` (`id` PK, `embedding` vector(3072), `metadata` JSONB, `updated_at` TIMESTAMPTZ).                                  |
| FR5  | **Semantic search API**: `POST /search` body `{ "q": "text", "k": 20, "filters": { â€¦ } }` â†’ returns sorted highlights.                                               |
| FR6  | **Filters**: optional equality filters on `source`, `author`, `tags`, `highlighted_at` range.                                                                        |
| FR7  | **Observability**: `/health` endpoint, structured logs, Promâ€‘style metrics counter for rows synced & errors.                                                         |
| FR8  | **CLI commands**: `sync --backfill`, `sync --since ISO`, `search "query"`.                                                                                           |
| FR9  | **Scheduler**: GitHub Actions workflow `sync.yml` triggers daily at 03:00Â UTC with cache of Poetry deps.                                                             |
| FR10 | **MCP query server**: Provide a lightweight MCP server that LLM clients can connect to for realâ€‘time semantic search over the vector DB (streams `/search` results). |

---

## 8Â Nonâ€‘Functional Requirements

* **Performance**: â‰¤Â 500â€¯ms P95 search, â‰¤Â 5â€¯m full nightly sync on 5â€¯k new highlights.
* **Reliability**: Retry w/ exponential backoff for transient 5xx, rateâ€‘limit respect (Readwise 20Â req/min).
* **Security**: API key auth header, `.env` secrets not committed, TLS termination assumed by reverse proxy.
* **Portability**: Single `dockerâ€‘compose up` builds entire stack; volumes for Postgres data & embeddings model cached.
* **Maintainability**: Modular packages (`core/`, `api/`, `jobs/`), 90â€¯% mypy coverage, pytest suite.
* **Accessibility**: n/a (no UI).

---

## 9Â System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     nightly     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub     â”‚  cron trigger  â”‚  Sync Job  â”‚
â”‚  Actions    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Python)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚embeds
                    Readwise API     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  MCP  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    & OpenAI         â–¼           â–¼           â””â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ LLM Clientâ”‚
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP JSON â”‚ FastAPIâ”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Postgres â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  + MCP   â”‚
                                 â”‚+pgvector â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
                                                                      â–¼
                                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                 â”‚ MCP Serv. â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The **MCP Server** is a minimal stateless adapter that exposes the same semanticâ€‘search capability as FastAPI but speaks the lightweight **MCP protocol** required by CLine/Cursor LLM clients. It accepts a search query message, streams topâ€‘*k* results with metadata, and relays backâ€‘pressure signals so clients can paginate or cancel early.

---

## 10Â Data Model (DDL excerpt)

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE highlights (
  id           BIGINT PRIMARY KEY,
  text         TEXT NOT NULL,
  source_type  TEXT,          -- "book", "article", etc.
  source_id    TEXT,
  title        TEXT,
  author       TEXT,
  url          TEXT,
  tags         TEXT[],
  highlighted_at TIMESTAMPTZ,
  updated_at   TIMESTAMPTZ,
  embedding    vector(3072)   -- OpenAI textâ€‘embeddingâ€‘3â€‘large
);

CREATE TABLE sync_state (
  id               SMALLINT PRIMARY KEY DEFAULT 1,
  last_cursor      TEXT,
  last_synced_at   TIMESTAMPTZ
);

CREATE INDEX ON highlights USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);
```

---

## 11Â API Design

### 11.1Â `POST /search`

| Field     | Type   | Required | Notes                          |
| --------- | ------ | -------- | ------------------------------ |
| `q`       | string | âœ”        | search query                   |
| `k`       | int    | âœ– (20)   | number of results              |
| `filters` | object | âœ–        | `source`, `tags`, `date_range` |

**Response** `200Â OK`

```jsonc
{
  "results": [
    {
      "id": 12345,
      "text": "â€¦",
      "score": 0.87,
      "metadata": { "title": "â€¦", "author": "â€¦" }
    }
  ],
  "elapsed_ms": 42
}
```

### 11.2Â `GET /health`

Returns `200` with `{ "status": "ok" }` when DB reachable.

---

## 12Â TechÂ Stack & Tooling

* **Language**: PythonÂ 3.12, Poetry
* **Frameworks**: FastAPI, PydanticÂ v2, SQLModel/SQLAlchemyÂ 2
* **Database**: PostgresÂ 16, pgvectorÂ 0.6.1
* **Embeddings**: OpenAIÂ `textâ€‘embeddingâ€‘3â€‘large` via `openai==1.*`
* **Container**: DockerÂ 25, ComposeÂ v2
* **CI/CD**: GitHubÂ Actions; preâ€‘commit hooks (black, isort, ruff, mypy)
* **Monitoring**: Prometheus exporter (`prometheusâ€‘fastapiâ€‘instrumentator`), Grafana optional.

---

## 13Â Deployment & Operations

1. **Local dev**: `docker compose up db && poetry run uvicorn â€¦`
2. **Prodâ€‘like selfâ€‘host**: same compose; user maps portÂ 80 externally.
3. **Backâ€‘ups**: nightly `pg_dump` via cron; files copied to userâ€™s NAS.
4. **Secrets**: `.env` file mounted; never stored in repo.
5. **Upgrades**: semanticâ€‘versioned releases; run `alembic upgrade head` for schema changes.

---

## 14Â Risks & Mitigations

| Â Risk                   | Likelihood | Impact | Mitigation                                          |
| ----------------------- | ---------- | ------ | --------------------------------------------------- |
| OpenAI price change     | Medium     | Medium | Switch to local MiniLM fallback layer + Faiss       |
| Readwise API quota cuts | Low        | High   | Cache pages, exponential backoff, email alerts      |
| Local disk full         | Medium     | High   | Send warning when `pg_total_relation_size` >Â 400Â MB |

---

## 15Â Metrics & KPIs

* **rows\_synced\_total** counter
* **sync\_duration\_seconds** histogram
* **search\_latency\_ms** P95 gauge
* **error\_rate** (sync + search)

---

## 16Â Roadmap / Milestones

| Â Phase   | Target date | Deliverables                                           |
| -------- | ----------- | ------------------------------------------------------ |
| Â MVPÂ Î±   | 07â€‘07â€‘2025  | Backâ€‘fill script + Postgres schema + manual search CLI |
| Â MVPÂ Î²   | 21â€‘07â€‘2025  | FastAPI `/search`, nightly GitHub Actions sync         |
| Â v1.0Â GA | 15â€‘08â€‘2025  | Observability, error alerts, README install guide      |
| Â v1.1    | 01â€‘10â€‘2025  | Local fallback embeddings, simple React UI             |

---

## 17Â Outâ€‘ofâ€‘Scope (v1)

* WebÂ UI with authentication
* Crossâ€‘device sync UI
* Advanced ranking (RAG / LLM reranker)
* PDF/page screenshot storage

---

## 18Â Glossary

* **Embedding**Â â€“ fixedâ€‘size dense vector representing semantic meaning of text.
* **pgvector**Â â€“ PostgreSQL extension providing vector types and distance operators.
* **IVFFlat**Â â€“ Index method for efficient approximate nearestâ€‘neighbor search.
* **Readwise Reader**Â â€“ Readwiseâ€™s readâ€‘itâ€‘later app with separate API (`/v3`).

---

### End of Document
